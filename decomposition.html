<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.4 Functional Decompositon | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="8.4 Functional Decompositon | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.4 Functional Decompositon | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interaction.html"/>
<link rel="next" href="feature-importance.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>


<a id="cta-button-desktop" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank">Buy</a>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i><b>1</b> Preface by the Author</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b> Story Time</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="2.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>2.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="3.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b> Algorithm Transparency</a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>3.4</b> Evaluation of Interpretability</a></li>
<li class="chapter" data-level="3.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>3.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="3.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.6.1</b> What Is an Explanation?</a></li>
<li class="chapter" data-level="3.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.6.2</b> What Is a Good Explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Datasets</a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b> Bike Rentals (Regression)</a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable Models</a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>5.1.2</b> Example</a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>5.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>5.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>5.1.8</b> Advantages</a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>5.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.2.1</b> What is Wrong with Linear Regression for Classification?</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>5.2.2</b> Theory</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>5.2.4</b> Example</a></li>
<li class="chapter" data-level="5.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>5.2.5</b> Advantages and Disadvantages</a></li>
<li class="chapter" data-level="5.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>5.2.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>5.3</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="5.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>5.3.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="5.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>5.3.2</b> Interactions</a></li>
<li class="chapter" data-level="5.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>5.3.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="5.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>5.3.4</b> Advantages</a></li>
<li class="chapter" data-level="5.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>5.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>5.3.6</b> Software</a></li>
<li class="chapter" data-level="5.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>5.3.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>5.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="5.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>5.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>5.4.2</b> Example</a></li>
<li class="chapter" data-level="5.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>5.4.3</b> Advantages</a></li>
<li class="chapter" data-level="5.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>5.4.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>5.4.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>5.5</b> Decision Rules</a><ul>
<li class="chapter" data-level="5.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>5.5.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="5.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>5.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="5.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>5.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="5.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>5.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>5.6</b> RuleFit</a><ul>
<li class="chapter" data-level="5.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>5.6.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="5.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>5.6.2</b> Theory</a></li>
<li class="chapter" data-level="5.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>5.6.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>5.7</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="5.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.7.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> Model-Agnostic Methods</a></li>
<li class="chapter" data-level="7" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>7</b> Example-Based Explanations</a></li>
<li class="chapter" data-level="8" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>8</b> Global Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="8.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>8.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="pdp.html"><a href="pdp.html#pdp-based-feature-importance"><i class="fa fa-check"></i><b>8.1.1</b> PDP-based Feature Importance</a></li>
<li class="chapter" data-level="8.1.2" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>8.1.2</b> Examples</a></li>
<li class="chapter" data-level="8.1.3" data-path="pdp.html"><a href="pdp.html#advantages-5"><i class="fa fa-check"></i><b>8.1.3</b> Advantages</a></li>
<li class="chapter" data-level="8.1.4" data-path="pdp.html"><a href="pdp.html#disadvantages-5"><i class="fa fa-check"></i><b>8.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.1.5" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>8.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>8.2</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>8.2.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="8.2.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>8.2.2</b> Theory</a></li>
<li class="chapter" data-level="8.2.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>8.2.3</b> Estimation</a></li>
<li class="chapter" data-level="8.2.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
<li class="chapter" data-level="8.2.5" data-path="ale.html"><a href="ale.html#advantages-6"><i class="fa fa-check"></i><b>8.2.5</b> Advantages</a></li>
<li class="chapter" data-level="8.2.6" data-path="ale.html"><a href="ale.html#disadvantages-6"><i class="fa fa-check"></i><b>8.2.6</b> Disadvantages</a></li>
<li class="chapter" data-level="8.2.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>8.2.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>8.3</b> Feature Interaction</a><ul>
<li class="chapter" data-level="8.3.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>8.3.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="8.3.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="8.3.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>8.3.3</b> Examples</a></li>
<li class="chapter" data-level="8.3.4" data-path="interaction.html"><a href="interaction.html#advantages-7"><i class="fa fa-check"></i><b>8.3.4</b> Advantages</a></li>
<li class="chapter" data-level="8.3.5" data-path="interaction.html"><a href="interaction.html#disadvantages-7"><i class="fa fa-check"></i><b>8.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="8.3.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>8.3.6</b> Implementations</a></li>
<li class="chapter" data-level="8.3.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>8.3.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>8.4</b> Functional Decompositon</a><ul>
<li class="chapter" data-level="8.4.1" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-i"><i class="fa fa-check"></i><b>8.4.1</b> How not to Compute the Components I</a></li>
<li class="chapter" data-level="8.4.2" data-path="decomposition.html"><a href="decomposition.html#functional-decomposition"><i class="fa fa-check"></i><b>8.4.2</b> Functional Decomposition</a></li>
<li class="chapter" data-level="8.4.3" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-ii"><i class="fa fa-check"></i><b>8.4.3</b> How not to Compute the Components II</a></li>
<li class="chapter" data-level="8.4.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>8.4.4</b> Functional ANOVA</a></li>
<li class="chapter" data-level="8.4.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>8.4.5</b> Generalized Functional ANOVA for Dependent Features</a></li>
<li class="chapter" data-level="8.4.6" data-path="decomposition.html"><a href="decomposition.html#accumulated-local-effect-plots"><i class="fa fa-check"></i><b>8.4.6</b> Accumulated Local Effect Plots</a></li>
<li class="chapter" data-level="8.4.7" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>8.4.7</b> Statistical Regression Models</a></li>
<li class="chapter" data-level="8.4.8" data-path="decomposition.html"><a href="decomposition.html#bonus-partial-dependence-plot"><i class="fa fa-check"></i><b>8.4.8</b> Bonus: Partial Dependence Plot</a></li>
<li class="chapter" data-level="8.4.9" data-path="decomposition.html"><a href="decomposition.html#advantages-8"><i class="fa fa-check"></i><b>8.4.9</b> Advantages</a></li>
<li class="chapter" data-level="8.4.10" data-path="decomposition.html"><a href="decomposition.html#disadvantages-8"><i class="fa fa-check"></i><b>8.4.10</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>8.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="8.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>8.5.1</b> Theory</a></li>
<li class="chapter" data-level="8.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>8.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="8.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>8.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="8.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-9"><i class="fa fa-check"></i><b>8.5.4</b> Advantages</a></li>
<li class="chapter" data-level="8.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-9"><i class="fa fa-check"></i><b>8.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="8.5.6" data-path="feature-importance.html"><a href="feature-importance.html#alternatives-1"><i class="fa fa-check"></i><b>8.5.6</b> Alternatives</a></li>
<li class="chapter" data-level="8.5.7" data-path="feature-importance.html"><a href="feature-importance.html#software-3"><i class="fa fa-check"></i><b>8.5.7</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>8.6</b> Global Surrogate</a><ul>
<li class="chapter" data-level="8.6.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>8.6.1</b> Theory</a></li>
<li class="chapter" data-level="8.6.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>8.6.2</b> Example</a></li>
<li class="chapter" data-level="8.6.3" data-path="global.html"><a href="global.html#advantages-10"><i class="fa fa-check"></i><b>8.6.3</b> Advantages</a></li>
<li class="chapter" data-level="8.6.4" data-path="global.html"><a href="global.html#disadvantages-10"><i class="fa fa-check"></i><b>8.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.6.5" data-path="global.html"><a href="global.html#software-4"><i class="fa fa-check"></i><b>8.6.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>8.7</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="8.7.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>8.7.1</b> Theory</a></li>
<li class="chapter" data-level="8.7.2" data-path="proto.html"><a href="proto.html#examples-3"><i class="fa fa-check"></i><b>8.7.2</b> Examples</a></li>
<li class="chapter" data-level="8.7.3" data-path="proto.html"><a href="proto.html#advantages-11"><i class="fa fa-check"></i><b>8.7.3</b> Advantages</a></li>
<li class="chapter" data-level="8.7.4" data-path="proto.html"><a href="proto.html#disadvantages-11"><i class="fa fa-check"></i><b>8.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.7.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>8.7.5</b> Code and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>9</b> Local Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>9.1</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ice.html"><a href="ice.html#examples-4"><i class="fa fa-check"></i><b>9.1.1</b> Examples</a></li>
<li class="chapter" data-level="9.1.2" data-path="ice.html"><a href="ice.html#advantages-12"><i class="fa fa-check"></i><b>9.1.2</b> Advantages</a></li>
<li class="chapter" data-level="9.1.3" data-path="ice.html"><a href="ice.html#disadvantages-12"><i class="fa fa-check"></i><b>9.1.3</b> Disadvantages</a></li>
<li class="chapter" data-level="9.1.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>9.1.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>9.2</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>9.2.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>9.2.2</b> LIME for Text</a></li>
<li class="chapter" data-level="9.2.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>9.2.3</b> LIME for Images</a></li>
<li class="chapter" data-level="9.2.4" data-path="lime.html"><a href="lime.html#advantages-13"><i class="fa fa-check"></i><b>9.2.4</b> Advantages</a></li>
<li class="chapter" data-level="9.2.5" data-path="lime.html"><a href="lime.html#disadvantages-13"><i class="fa fa-check"></i><b>9.2.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>9.3</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="9.3.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>9.3.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="9.3.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>9.3.2</b> Example</a></li>
<li class="chapter" data-level="9.3.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-14"><i class="fa fa-check"></i><b>9.3.3</b> Advantages</a></li>
<li class="chapter" data-level="9.3.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-14"><i class="fa fa-check"></i><b>9.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="9.3.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>9.3.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>9.4</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>9.4.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="9.4.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>9.4.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="9.4.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>9.4.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="9.4.4" data-path="anchors.html"><a href="anchors.html#advantages-15"><i class="fa fa-check"></i><b>9.4.4</b> Advantages</a></li>
<li class="chapter" data-level="9.4.5" data-path="anchors.html"><a href="anchors.html#disadvantages-15"><i class="fa fa-check"></i><b>9.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="9.4.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>9.4.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9.5</b> Shapley Values</a><ul>
<li class="chapter" data-level="9.5.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>9.5.1</b> General Idea</a></li>
<li class="chapter" data-level="9.5.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>9.5.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="9.5.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>9.5.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="9.5.4" data-path="shapley.html"><a href="shapley.html#advantages-16"><i class="fa fa-check"></i><b>9.5.4</b> Advantages</a></li>
<li class="chapter" data-level="9.5.5" data-path="shapley.html"><a href="shapley.html#disadvantages-16"><i class="fa fa-check"></i><b>9.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="9.5.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>9.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>9.6</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="9.6.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>9.6.1</b> Definition</a></li>
<li class="chapter" data-level="9.6.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>9.6.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="9.6.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>9.6.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="9.6.4" data-path="shap.html"><a href="shap.html#examples-5"><i class="fa fa-check"></i><b>9.6.4</b> Examples</a></li>
<li class="chapter" data-level="9.6.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>9.6.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="9.6.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>9.6.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="9.6.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>9.6.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="9.6.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>9.6.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="9.6.9" data-path="shap.html"><a href="shap.html#clustering-shapley-values"><i class="fa fa-check"></i><b>9.6.9</b> Clustering Shapley Values</a></li>
<li class="chapter" data-level="9.6.10" data-path="shap.html"><a href="shap.html#advantages-17"><i class="fa fa-check"></i><b>9.6.10</b> Advantages</a></li>
<li class="chapter" data-level="9.6.11" data-path="shap.html"><a href="shap.html#disadvantages-17"><i class="fa fa-check"></i><b>9.6.11</b> Disadvantages</a></li>
<li class="chapter" data-level="9.6.12" data-path="shap.html"><a href="shap.html#software-5"><i class="fa fa-check"></i><b>9.6.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>10</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="10.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>10.1</b> Learned Features</a><ul>
<li class="chapter" data-level="10.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>10.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="10.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>10.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="10.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>10.1.3</b> Advantages</a></li>
<li class="chapter" data-level="10.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>10.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="10.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>10.1.5</b> Software and Further Material</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>10.2</b> Pixel Attribution (Saliency Maps)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>10.2.1</b> Vanilla Gradient (Saliency Maps)</a></li>
<li class="chapter" data-level="10.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>10.2.2</b> DeconvNet</a></li>
<li class="chapter" data-level="10.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>10.2.3</b> Grad-CAM</a></li>
<li class="chapter" data-level="10.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>10.2.4</b> Guided Grad-CAM</a></li>
<li class="chapter" data-level="10.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>10.2.5</b> SmoothGrad</a></li>
<li class="chapter" data-level="10.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>10.2.6</b> Examples</a></li>
<li class="chapter" data-level="10.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>10.2.7</b> Advantages</a></li>
<li class="chapter" data-level="10.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>10.2.8</b> Disadvantages</a></li>
<li class="chapter" data-level="10.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-6"><i class="fa fa-check"></i><b>10.2.9</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>10.3</b> Detecting Concepts</a><ul>
<li class="chapter" data-level="10.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>10.3.1</b> TCAV: Testing with Concept Activation Vectors</a></li>
<li class="chapter" data-level="10.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>10.3.2</b> Example</a></li>
<li class="chapter" data-level="10.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>10.3.3</b> Advantages</a></li>
<li class="chapter" data-level="10.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>10.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="10.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>10.3.5</b> Bonus: Other Concept-based Approaches</a></li>
<li class="chapter" data-level="10.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-7"><i class="fa fa-check"></i><b>10.3.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>10.4</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="10.4.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>10.4.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="10.4.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>10.4.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>10.5</b> Influential Instances</a><ul>
<li class="chapter" data-level="10.5.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>10.5.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="10.5.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>10.5.2</b> Influence Functions</a></li>
<li class="chapter" data-level="10.5.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="10.5.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="10.5.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>10.5.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>11</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="11.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>11.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="11.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>11.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>12</b> Contribute to the Book</a></li>
<li class="chapter" data-level="13" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>13</b> Citing this Book</a></li>
<li class="chapter" data-level="14" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>14</b> Translations</a></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used.html"><a href="r-packages-used.html"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="decomposition" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.4</span> Functional Decompositon<a href="decomposition.html#decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A supervised machine learning model can be viewed as a function that takes a high-dimensional feature vector as input and produces a prediction or classification score as output.
Functional decomposition is an interpretation technique that deconstructs the high-dimensional function and expresses it as a sum of individual feature effects and interaction effects that can be visualized.
In addition, functional decomposition is a fundamental principle underlying many interpretation techniques – it helps you better understand other interpretation methods.</p>
<!-- Intuition -->
<p>Let us jump right in and look at a particular function.
This function takes two features as input and produces a one-dimensional output:</p>
<p><span class="math display">\[y = \hat{f}(x_1, x_2) = 2 + e^{x_1} - x_2 + x_1 \cdot x_2\]</span></p>
<p>Think of the function as a machine learning model.
We can visualize the function with a 3D plot or a heatmap with contour lines:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="images/unnamed-chunk-16-1.png" alt="Prediction surface of a function with two features $X_1$ and $X_2$." width="\textwidth" />
<p class="caption">
FIGURE 8.22: Prediction surface of a function with two features <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.
</p>
</div>
<!-- Describing the prediction function -->
<p>The function takes large values when <span class="math inline">\(X_1\)</span> is large and <span class="math inline">\(X_2\)</span> is small, and it takes small values for large <span class="math inline">\(X_2\)</span> and small <span class="math inline">\(X_1\)</span>.
The prediction function is not simply an additive effect between the two features, but an interaction between the two.
The presence of an interaction can be seen in the figure – the effect of changing values for feature <span class="math inline">\(X_1\)</span> depends on the value that feature <span class="math inline">\(X_2\)</span> has.</p>
<!-- Referring back to functional decomposition for ML -->
<p>Our job now is to decompose this function into main effects of features <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and an interaction term.
For a two-dimensional function <span class="math inline">\(\hat{f}\)</span> that depends on only two input features: <span class="math inline">\(\hat{f}(x_1, x_2)\)</span>, we want each component to represent a main effect (<span class="math inline">\(\hat{f}_1\)</span> and <span class="math inline">\(\hat{f}_2\)</span>), interaction (<span class="math inline">\(\hat{f}_{1,2}\)</span>) or intercept (<span class="math inline">\(\hat{f}_0\)</span>):</p>
<p><span class="math display">\[\hat{f}(x_1, x_2) = \hat{f}_0 + \hat{f}_1(x_1) + \hat{f}_2(x_2) + \hat{f}_{1,2}(x_{1},x_{2})\]</span></p>
<p>The main effects indicate how each feature affects the prediction, independent of the values the other feature.
The interaction effect indicates the joint effect of the features.
The intercept simply tells us what the prediction is when all feature effects are set to zero.
Note that the components themselves are functions (except for the intercept) with different input dimensionality.</p>
<p>I will just give you the components now and explain where they come from later.
The intercept is <span class="math inline">\(\hat{f}_0\sim3.18\)</span>.
Since the other components are functions, we can visualize them:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="images/unnamed-chunk-17-1.png" alt="Decomposition of a function." width="\textwidth" />
<p class="caption">
FIGURE 8.23: Decomposition of a function.
</p>
</div>
<p>Do you think the components make sense given the above true formula, ignoring that the intercept value seems a bit random?
The <span class="math inline">\(x_1\)</span> feature shows an exponential main effect, and <span class="math inline">\(x_2\)</span> shows a negative linear effect.
The interaction term looks a bit like a Pringles chip.
In less crunchy and more mathematical terms, it is a hyperbolic paraboloid, as we would expect for <span class="math inline">\(x_1 \cdot x_2\)</span>.
Spoiler alert: the decomposition is based on accumulated local effect plots, which we will discuss later in the chapter.</p>
<div id="how-not-to-compute-the-components-i" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.1</span> How not to Compute the Components I<a href="decomposition.html#how-not-to-compute-the-components-i" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- The naive approach -->
<p>But why all the excitement?
A glance at the formula already gives us the answer to the decomposition, so no need for fancy methods, right?
For feature <span class="math inline">\(x_1\)</span>, we can take all the summands that contain only <span class="math inline">\(x_1\)</span> as the component for that feature.
That would be <span class="math inline">\(\hat{f}_1(x_1) = e^{x_1}\)</span> and <span class="math inline">\(\hat{f}_2(x_2) = -x_2\)</span> for feature <span class="math inline">\(x_2\)</span>.
The interaction is then <span class="math inline">\(\hat{f}_{12}(x_{1},x_{2}) = x_1 \cdot x_2\)</span>.
While this is the correct answer for this example (up to constants), there are two problems with this approach:
Problem 1): While the example started with the formula, the reality is that almost no machine learning model can be described with such a neat formula.
Problem 2) is much more intricate and concerns what an interaction is.
Imagine a simple function <span class="math inline">\(\hat{f}(x_1,x_2) = x_1 \cdot x_2\)</span>, where both features take values larger than zero and are independent of each other.
Using our look-at-the-formula tactic, we would conclude that there is an interaction between features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, but not individual feature effects.
But can we really say that feature <span class="math inline">\(x_1\)</span> has no individual effect on the prediction function?
Regardless of what value the other feature <span class="math inline">\(x_2\)</span> takes on, the prediction increases as we increase <span class="math inline">\(x_1\)</span>.
For example, for <span class="math inline">\(x_2 = 1\)</span>, the effect of <span class="math inline">\(x_1\)</span> is <span class="math inline">\(\hat{f}(x_1, 1) = x_1\)</span>, and when <span class="math inline">\(x_2 = 10\)</span> the effect is <span class="math inline">\(\hat{f}(x_1, 10) = 10 \cdot x_1\)</span>.
Thus, it is clear that feature <span class="math inline">\(x_1\)</span> has a positive effect on the prediction, independent of <span class="math inline">\(x_2\)</span>, and is not zero.</p>
<p>To solve problem 1) of lack of access to a neat formula, we need a method that uses only the prediction function or classification score.
To solve problem 2) of lack of definition, we need some axioms that tell us what the components should look like and how they relate to each other.
But first, we should define more precisely what functional decomposition is.</p>
</div>
<div id="functional-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.2</span> Functional Decomposition<a href="decomposition.html#functional-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Explaining the formula -->
<p>A prediction function takes <span class="math inline">\(p\)</span> features as input, <span class="math inline">\(\hat{f}: \mathbb{R}^p \mapsto \mathbb{R}\)</span> and produces an output.
This can be a regression function, but it can also be the classification probability for a given class or the score for a given cluster (unsupervised machine learning).
Fully decomposed, we can represent the prediction function as the sum of functional components:</p>
<p><span class="math display">\[\begin{align*}
\hat{f}(x) = &amp; \hat{f}_0 + \hat{f}_1(x_1) + \ldots + \hat{f}_p(x_p) \\
&amp; + \hat{f}_{1,2}(x_1, x_2) + \ldots + \hat{f}_{1,p}(x_1, x_p) + \ldots + \hat{f}_{p-1,p}(x_{p-1}, x_p) \\ 
&amp; + \ldots  \\ &amp; +  \hat{f}_{1,\ldots,p}(x_1, \ldots, x_p)
\end{align*}\]</span></p>
<p>We can make the decomposition formula a bit nicer by indexing all possible subsets of feature combinations: <span class="math inline">\(S\subseteq\{1,\ldots,p\}\)</span>.
This set contains the intercept (<span class="math inline">\(S=\emptyset\)</span>), main effects (<span class="math inline">\(|S|=1\)</span>), and all interactions (<span class="math inline">\(|S|\geq{}1\)</span>).
With this subset defined, we can write the decomposition as follows:</p>
<p><span class="math display">\[\hat{f}(x) = \sum_{S\subseteq\{1,\ldots,p\}} \hat{f}_S(x_S)\]</span></p>
<p>In the formula, <span class="math inline">\(x_S\)</span> is the vector of features in the index set <span class="math inline">\(S\)</span>.
And each subset <span class="math inline">\(S\)</span> represents a functional component, for example a main effect if S contains only one feature, or an interaction if <span class="math inline">\(|S| &gt; 1\)</span>.</p>
<!-- Dimensionality -->
<p>How many components are in the above formula?
The answer boils down to how many possible subsets <span class="math inline">\(S\)</span> of the features <span class="math inline">\(1,\ldots, p\)</span> we can form.
And these are <span class="math inline">\(\sum_{i=0}^p\binom{p}{i}=2^p\)</span> possible subsets!
For example, if a function uses 10 features, we can decompose the function into 1042 components: 1 intercept, 10 main effects, 90 2-way interaction terms, 720 3-way interaction terms, …
And with each additional feature, the number of components doubles.
Clearly, for most functions, it is not feasible to compute all components.
Another reason NOT to compute all components is that components with <span class="math inline">\(|S|&gt;2\)</span> are difficult to visualize and interpret.</p>
</div>
<div id="how-not-to-compute-the-components-ii" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.3</span> How not to Compute the Components II<a href="decomposition.html#how-not-to-compute-the-components-ii" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far I have avoided talking about how the components are defined and computed.
The only constraints we have implicitly talked about were the number and dimensionality of the components, and that the sum of components should yield the original function.
But without further constraints on what the components should be, they are not unique.
This means we could shift effects between main effects and interactions, or lower-order interactions (few features) and higher-order interactions (more features).
In the example at the beginning of the chapter we could set both main effects to zero, and add their effects to the interaction effect.</p>
<p>Here is an even more extreme example that illustrates the need for constraints on components.
Suppose you have a 3-dimensional function.
It does not really matter what this function looks like, but the following decomposition would <strong>always</strong> work:
<span class="math inline">\(\hat{f}_0\)</span> is 0.12.
<span class="math inline">\(\hat{f}_1(x_1)=2\cdot{}x_1\)</span> + number of shoes you own.
<span class="math inline">\(\hat{f}_2\)</span>, <span class="math inline">\(\hat{f}_3\)</span>, <span class="math inline">\(\hat{f}_{1,2}\)</span>, <span class="math inline">\(\hat{f}_{2,3}, \hat{f}_{1,3}\)</span> are all zero.
And to make this trick work, I define <span class="math inline">\(\hat{f}_{1,2,3}(x_1,x_2,x_3)=\hat{f}(x)-\sum_{S\subset\{1,\ldots,p\}}\hat{f}_S(x_S)\)</span>.
So the interaction term containing all features simply sucks up all the remaining effects, which by definition always works, in the sense that the sum of all components gives us the original prediction function.
This decomposition would not be very meaningful and quite misleading if you were to present this as the interpretation of your model.</p>
<!-- Solutions -->
<p>The ambiguity can be avoided by specifying further constraints or specific methods for computing the components.
In this chapter, we will discuss three methods that approach the functional decomposition in different ways:</p>
<ul>
<li>(Generalized) Functional ANOVA</li>
<li><a href="ale.html#ale">Accumulated Local Effects</a></li>
<li>Statistical regression models</li>
</ul>
</div>
<div id="functional-anova" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.4</span> Functional ANOVA<a href="decomposition.html#functional-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Functional ANOVA was proposed by Hooker (2004)<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a>.
A requirement for this approach is that the model prediction function <span class="math inline">\(\hat{f}\)</span> is square integrable.
As with any functional decomposition, the functional ANOVA decomposes the function into components:</p>
<p><span class="math display">\[\hat{f}(x) = \sum_{S\subseteq\{1,\ldots,p\}} \hat{f}_S(x_S)\]</span></p>
<p>Hooker (2004) defines each component with the following formula:</p>
<p><span class="math display">\[\hat{f}_S(x) = \int_{X_{-S}} \left( \hat{f}(x) - \sum_{V \subset S} \hat{f}_V(x)\right) d X_{-S}\]</span></p>
<p>Okay, let us take this thing apart.
We can rewrite the component as:</p>
<p><span class="math display">\[\hat{f}_S(x) = \int_{X_{-S}} \left( \hat{f}(x)\right) d X_{-S} - \int_{X_{-S}} \left(\sum_{V \subset S} \hat{f}_V(x) \right) d X_{-S}\]</span></p>
<p>On the left side is the integral over the prediction function with respect to the features excluded from the set <span class="math inline">\(S\)</span>, denoted with <span class="math inline">\(-S\)</span>.
For example, if we compute the 2-way interaction component for features 2 and 3, we would integrate over features 1, 4, 5, …
The integral can also be viewed as the expected value of the prediction function with respect to <span class="math inline">\(X_{-S}\)</span>, assuming that all features follow a uniform distribution from their minimum to their maximum.
From this interval, we subtract all components with subsets of <span class="math inline">\(S\)</span>.
This subtraction removes the effect of all lower-order effects and centers the effect.
For <span class="math inline">\(S=\{1,2\}\)</span>, we subtract the main effects of both features <span class="math inline">\(\hat{f}_1\)</span> and <span class="math inline">\(\hat{f}_2\)</span>, as well as the intercept <span class="math inline">\(\hat{f}_0\)</span>.
The occurrence of these lower-order effects makes the formula recursive:
We have to go through the hierarchy of subsets to the intercept and compute all these components.
For the intercept component <span class="math inline">\(\hat{f}_0\)</span>, the subset is the empty set <span class="math inline">\(S=\{\emptyset\}\)</span> and therefore <span class="math inline">\(-S\)</span> contains all features:</p>
<p><span class="math display">\[\hat{f}_0(x) = \int_{X} \hat{f}(x) dX\]</span></p>
<p>This is simply the prediction function integrated over all features.
The intercept can also be interpreted as the expectation of the prediction function when we assume that all features are uniformly distributed.
Now that we know <span class="math inline">\(\hat{f}_0\)</span>, we can compute <span class="math inline">\(\hat{f}_1\)</span> (and equivalently <span class="math inline">\(\hat{f}_2\)</span>):</p>
<p><span class="math display">\[\hat{f}_1(x) = \int_{X_{-1}} \left( \hat{f}(x) - \hat{f}_0\right) d X_{-S}\]</span></p>
<p>To finish the calculation for the component <span class="math inline">\(\hat{f}_{1,2}\)</span>, we can put everything together:</p>
<p><span class="math display">\[\begin{align*}\hat{f}_{1,2}(x) &amp;= \int_{X_{3,4}} \left( \hat{f}(x) - (\hat{f}_0(x) + \hat{f}_1(x) - \hat{f}_0 + \hat{f}_2(x) - \hat{f}_0)\right) d X_{3},X_4 \\  &amp;= \int_{X_{3,4}} \left(\hat{f}(x) - \hat{f}_1(x) - \hat{f}_2(x) + \hat{f}_0\right) d X_{3},X_4 \end{align*}\]</span></p>
<p>This example shows how each higher-order effect is defined by integrating over all other features, but also by removing all the lower-order effects that are subsets of the feature set we are interested in.</p>
<p>Hooker (2004) has shown that this definition of functional components satisfies these desirable axioms:</p>
<ul>
<li>Zero Means: <span class="math inline">\(\int{}\hat{f}_S(x_S)dX_s=0\)</span> for each <span class="math inline">\(S\neq\emptyset\)</span>.</li>
<li>Orthogonality: <span class="math inline">\(\int{}\hat{f}_S(x_S)\hat{f}_V(x_v)dX=0\)</span> for <span class="math inline">\(S\neq{}V\)</span></li>
<li>Variance Decomposition: Let <span class="math inline">\(\sigma^2_{\hat{f}}=\int \hat{f}(x)^2dX\)</span>, then <span class="math inline">\(\sigma^2(\hat{f}) = \sum_{S \subseteq \{1,\ldots,p\}} \sigma^2_S(\hat{f}_S)\)</span></li>
</ul>
<p>The zero means axiom implies that all effects or interactions are centered around zero.
As a consequence, the interpretation at a position x is relative to the centered prediction and not the absolute prediction.</p>
<p>The orthogonality axiom implies that components do not share information.
For example, the first-order effect of feature <span class="math inline">\(X_1\)</span> and the interaction term of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_2\)</span> are not correlated.
Because of orthogonality, all components are “pure” in the sense that they do not mix effects.
It makes a lot of sense that the component for, say, feature <span class="math inline">\(X_4\)</span> should be independent of the interaction term between features <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.
The more interesting consequence arises for orthogonality of hierarchical components, where one component contains features of another, for example the interaction between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, and the main effect of feature <span class="math inline">\(X_1\)</span>.
In contrast, a two-dimensional partial dependence plot for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> would contain four effects: the intercept, the two main effects of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and the interaction between them.
The functional ANOVA component for <span class="math inline">\(\hat{f}_{1,2}(x_1,x_2)\)</span> contains only the pure interaction.</p>
<p>Variance decomposition allows us to divide the variance of the function <span class="math inline">\(\hat{f}\)</span> among the components, and guarantees that it adds up the total variance of the function in the end.
The variance decomposition property can also explain to us why the method is called ``functional ANOVA’’.
In statistics, ANOVA stands for ANalysis Of VAriance.
ANOVA refers to a collection of methods that analyze differences in the mean of a target variable.
ANOVA works by dividing the variance and attributing it to the variables.
Functional ANOVA can therefore be seen as an extension of this concept to any function.</p>
<p>Problems arise with the functional ANOVA when features are correlated.
As a solution, the generalized functional ANOVA has been proposed.</p>
</div>
<div id="generalized-functional-anova-for-dependent-features" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.5</span> Generalized Functional ANOVA for Dependent Features<a href="decomposition.html#generalized-functional-anova-for-dependent-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- The extrapolation problem -->
<p>Similar to most interpretation techniques based on sampling data (such as the PDP), the functional ANOVA can produce misleading results when features are correlated.
If we integrate over the uniform distribution, when in reality features are dependent, we create a new dataset that deviates from the joint distribution and extrapolates to unlikely combinations of feature values.</p>
<!-- The solution -->
<p>Hooker (2007) <a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a> proposed the generalized functional ANOVA, a decomposition that works for dependent features.
It is a generalization of the functional ANOVA we encountered earlier, which means that the functional ANOVA is a special case of the generalized functional ANOVA.
The components are defined as projections of f onto the space of additive functions:</p>
<p><span class="math display">\[\hat{f}_S(x_S) = argmin_{g_S \in L^2(\mathbb{R}^S)_{S \in P}} \int \left(\hat{f}(x)  - \sum_{S \subset P} g_S(x_S)\right)^2 w(x)dx.\]</span></p>
<p>Instead of orthogonality, the components satisfy a hierarchical orthogonality condition:</p>
<p><span class="math display">\[\forall \hat{f}_S(x_S)| S \subset U: \int \hat{f}_S(x_S) \hat{f}_U(x_U) w(x)dx = 0\]</span></p>
<!-- hierarchical orthogonality -->
<p>Hierarchical orthogonality is different from orthogonality.
For two feature sets S and U, neither of which is the subset of the other (e.g. <span class="math inline">\(S=\{1,2\}\)</span> and <span class="math inline">\(U=\{2,3\}\)</span>), the components <span class="math inline">\(\hat{f}_S\)</span> and <span class="math inline">\(\hat{f}_U\)</span> need not be orthogonal for the decomposition to be hierarchically orthogonal.
But all components for all subsets of <span class="math inline">\(S\)</span> must be orthogonal to <span class="math inline">\(\hat{f}_S\)</span>.
<!-- Entanglement -->
As a result, the interpretation differs in relevant ways:
Similar to the M-Plot in the <a href="ale.html#ale">ALE chapter</a>, generalized functional ANOVA components can entangle the (marginal) effects of correlated features.
Whether the components entangle the marginal effects also depends on the choice of weight function <span class="math inline">\(w(x)\)</span>.
If we choose w to be the uniform measure on the unit cube, we obtain the functional ANOVA from the section above.
A natural choice for w is the joint probability distribution function.
However, the joint distribution is usually unknown, and difficult to estimate.
A trick can be to start with the uniform measure on the unit cube, and cut out areas without data.</p>
<p>The estimation is done on a grid of points in the feature space and is stated as a minimization problem that can be solved using regression techniques.
However, the components cannot be computed individually, nor hierarchically, but a complex system of equations involving other components has to be solved.
The computation is therefore quite complex and computationally intensive.</p>
</div>
<div id="accumulated-local-effect-plots" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.6</span> Accumulated Local Effect Plots<a href="decomposition.html#accumulated-local-effect-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>ALE plots (Apley and Zhu 2020<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>) also provide a functional decomposition, meaning that adding all ALE plots from intercept, 1D ALE plots, 2D ALE plots and so on, yields the prediction function.
ALE differs from the (generalized) functional ANOVA, as the components are not orthogonal but, as the authors call it, pseudo-orthogonal.
To understand pseudo-orthogonality, we have to define the operator <span class="math inline">\(H_S\)</span>, which takes a function <span class="math inline">\(\hat{f}\)</span> and maps it to its ALE plot for feature subset <span class="math inline">\(S\)</span>.
For example, the operator <span class="math inline">\(H_{1,2}\)</span> takes as input a machine learning model and produces the 2D ALE plot for features 1 and 2: <span class="math inline">\(H_{1,2}(\hat{f}) = \hat{f}_{ALE,12}\)</span>.
If we apply the same operator twice, we get the same ALE plot.
After applying the operator <span class="math inline">\(H_{1,2}\)</span> to <span class="math inline">\(f\)</span> once, we get the 2D ALE plot <span class="math inline">\(\hat{f}_{ALE,12}\)</span>.
Then we apply the operator again, not to <span class="math inline">\(f\)</span> but to <span class="math inline">\(\hat{f}_{ALE,12}\)</span>.
This is possible because the 2D ALE component is itself a function.
The result is again <span class="math inline">\(\hat{f}_{ALE,12}\)</span>, meaning we can apply the same operator several times and always get the same ALE plot.
This is the first part of pseudo-orthogonality.
But what is the result if we apply two different operators for different feature sets?
For example, <span class="math inline">\(H_{1,2}\)</span> and <span class="math inline">\(H_{1}\)</span>, or <span class="math inline">\(H_{1,2}\)</span> and <span class="math inline">\(H_{3,4,5}\)</span>?
The answer is zero.
If we first apply the ALE operator <span class="math inline">\(H_S\)</span> to a function and then apply <span class="math inline">\(H_U\)</span> to the result (with <span class="math inline">\(S \neq U\)</span>), then the result is zero.
In other words, the ALE plot of an ALE plot is zero, unless you apply the same ALE plot twice.
Or in other words, the ALE plot for the feature set S does not contain any other ALE plots in it.
Or in mathematical terms, the ALE operator maps functions to orthogonal subspaces of an inner product space.</p>
<p>As Apley and Zhu (2020) note, pseudo-orthogonality may be more desirable than hierarchical orthogonality, because it does not entangle marginal effects of the features.
Furthermore, ALE does not require estimation of the joint distribution; the components can be estimated in a hierarchical manner, which means that calculating the 2D ALE for features 1 and 2 requires only the calculations of individual ALE components of 1 and 2 and the intercept term in addition.</p>
</div>
<div id="statistical-regression-models" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.7</span> Statistical Regression Models<a href="decomposition.html#statistical-regression-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This approach ties in with <a href="simple.html#simple">interpretable models</a>, in particular <a href="extend-lm.html#extend-lm">generalized additive models</a>.
Instead of decomposing a complex function, we can build constraints into the modeling process so that we can easily read out the individual components.
While decomposition can be handled in a top-down manner, where we start with a high-dimensional function and decompose it, generalized additive models provide a bottom-up approach, where we build the model from simple components.
Both approaches have in common that their goal is to provide individual and interpretable components.
In statistical models, we restrict the number of components so that not all <span class="math inline">\(2^p\)</span> components have to be fitted.
The simplest version is linear regression:</p>
<p><span class="math display">\[\hat{f}(x) = \beta_0 + \beta_1 x_1 + \ldots \beta_p x_p\]</span></p>
<p>The formula looks very similar to the functional decomposition, but with two major modifications.
Modification 1: All interaction effects are excluded and we keep only intercept and main effects.
Modification 2: The main effects may only be linear in the features: <span class="math inline">\(\hat{f}_j(x_j)=\beta_j{}x_j\)</span>.
Viewing the linear regression model through the lens of functional decomposition, we see that the model itself represents a functional decomposition of the true function that maps from features to target, but under strong assumptions that the effects are linear effects and there are no interactions.</p>
<p>The generalized additive model relaxes the second assumption by allowing more flexible functions <span class="math inline">\(\hat{f}_j\)</span> through the use of splines.
Interactions can also be added, but this process is rather manual.
Approaches such as GA2M attempt to add 2-way interactions automatically to a GAM. <a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a></p>
<p>Thinking of a linear regression model or a GAM as functional decomposition can also lead to confusion.
If you apply the decomposition approaches from earlier in the chapter (generalized functional ANOVA and accumulated local effects), you may get components that are different from the components read directly from the GAM.
This can happen when interaction effects of correlated features are modeled in the GAM.
The discrepancy occurs because other functional decomposition approaches split effects differently between interactions and main effects.</p>
<p>So when should you use GAMs instead of a complex model + decomposition?
You should stick to GAMs when most interactions are zero, especially when there are no interactions with three or more features.
If we know that the maximum number of features involved in interactions is two (<span class="math inline">\(|S|\leq{}2\)</span>), then we can use approaches like MARS or GA2M.
Ultimately, model performance on test data may indicate whether a GAM is sufficient or a more complex model performs much better.</p>
<!--
### Viewing Other Methods Through the Lens of Decomposition

You might want to come back to this chapter again if you have a good grasp on some of the other methods.

First on a high level:
Feature effects are direct visualizations of the individual components.
However we have to distinguish between total effect and isolated effect for the higher-order feature effects.
PDP is total effect
ALE is individual effect
If you remove lower effects from PDP, you get fANOVA, at least for independent feature case.

PDP is a direct decomposition, but with additional intercept difference.
ALE is a decomposition.
For permutation feature importance,
Methods such as SHApley and co only describe a prediction with 1-dimensional effects.
What happened with the interaction terms? They are divided among the individual effects.
What happened in PFI with the interactions? They are also divided among individual effects.

The SHAP interaction plots can also be better understood through decompositions.


There are many methods that produce individual explanations in the form of $\hat{f}X)=\sum_{p=1}^n\phi_j$, which attribute one number per feature, see [Shapley Values](#shapley), [LIME](#lime) and most [pixel attribution methods for neural networks](#pixel-attribution), all of which you will encounter later in the book.
Looking at them through functional decomposition:
When methods fulfill the property of efficiency, like for example Shapley values do, it means that the sum of attributions are equal to the prediction.
This means that we decomposed the function.
But there are only first order effects, no interactions.
It means that all the interactions have to be split among the individual values per feature.
And we do not get separate interaction effects.


Can a function exist where lower-components are zero, but the interactions are non-zero?
For example, components for $X_1$ and $X_2$ are zero, but their interaction is not?
Yes!
For example, the XOR problem where $Y=X_1XOR{}X_2$.

-->
</div>
<div id="bonus-partial-dependence-plot" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.8</span> Bonus: Partial Dependence Plot<a href="decomposition.html#bonus-partial-dependence-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Does the <a href="pdp.html#pdp">Partial Dependence Plot</a> also provide a functional decomposition?
Short answer: No.
Longer answer:
The partial dependence plot for a feature set <span class="math inline">\(S\)</span> always contains all effects of the hierarchy – the PDP for <span class="math inline">\(\{1,2\}\)</span> contains not only the interaction, but also the individual feature effects.
As a consequence, adding all PDPs for all subsets does not yield the original function, and thus is not a valid decomposition.
But could we adjust the PDP, perhaps by removing all lower effects?
Yes, we could, but we would get something similar to the functional ANOVA.
However, instead of integrating over a uniform distribution, the PDP integrates over the marginal distribution of <span class="math inline">\(X_{-S}\)</span>, which is estimated using Monte Carlo sampling.</p>
</div>
<div id="advantages-8" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.9</span> Advantages<a href="decomposition.html#advantages-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I consider functional decomposition to be a <strong>core concept of machine learning interpretability</strong>.</p>
<p>Functional decomposition gives us a <strong>theoretical justification</strong> for decomposing high-dimensional and complex machine learning models into individual effects and interactions – a necessary step that allows us to interpret individual effects.
Functional decomposition is the core idea for techniques such as statistical regression models, ALE, (generalized) functional ANOVA, PDP, the H-statistic, and ICE curves.</p>
<p>Functional decomposition also provides a <strong>better understanding of other methods</strong>.
For example, <a href="feature-importance.html#feature-importance">permutation feature importance</a> breaks the association between a feature and the target.
Viewed through the functional decomposition lens, we can see that the permutation “destroys” the effect of all components in which the feature was involved.
This affects the main effect of the feature, but also all interactions with other features.
As another example, Shapley values decompose a prediction into additive effects of the individual feature.
But the functional decomposition tells us that there should also be interaction effects in the decomposition, so where are they?
Shapley values provide a fair attribution of effects to the individual features, meaning that all interactions are also fairly attributed to the features and therefore divided up among the Shapley values.</p>
<p>When considering functional decomposition as a tool, the use of <strong>ALE plots offers many advantages</strong>.
ALE plots provide a functional decomposition that is fast to compute, has software implementations (see the ALE chapter), and desirable pseudo-orthogonality properties.</p>
</div>
<div id="disadvantages-8" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.4.10</span> Disadvantages<a href="decomposition.html#disadvantages-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The concept of functional decomposition quickly reaches its <strong>limits for high-dimensional components</strong> beyond interactions between two features.
Not only does this exponential explosion in the number of features limit practicability, since we cannot easily visualize higher-order interactions, but computational time is insane if we were to compute all interactions.</p>
<p>Each method of functional decomposition method has their <strong>individual disadvantages</strong>.
The bottom-up approach – constructing regression models – is a quite manual process and imposes many constraints on the model that can affect predictive performance.
Functional ANOVA requires independent features.
Generalized functional ANOVA is very difficult to estimate.
Accumulated local effect plots do not provide a variance decomposition.</p>
<p>The functional decomposition approach is <strong>more appropriate for analyzing tabular data than text or images</strong>.</p>

<div style="page-break-after: always;"></div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="39">
<li id="fn39"><p>Hooker, Giles. “Discovering additive structure in black box functions.” Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. (2004).<a href="decomposition.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>Hooker, Giles. “Generalized functional anova diagnostics for high-dimensional functions of dependent variables.” Journal of Computational and Graphical Statistics 16.3 (2007): 709-732.<a href="decomposition.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p>Apley, Daniel W., and Jingyu Zhu. “Visualizing the effects of predictor variables in black box supervised learning models.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 82.4 (2020): 1059-1086.<a href="decomposition.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>Caruana, Rich, et al. “Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.” Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. (2015).<a href="decomposition.html#fnref42" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interaction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="feature-importance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/05-agnostic-decomposition.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["interpretable-ml.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
