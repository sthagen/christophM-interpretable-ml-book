<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-04-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shapley.html"/>
<link rel="next" href="neural-networks.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>


<a id="cta-button-desktop" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/interpretable-machine-learning" rel="noopener noreferrer" target="blank">Buy</a>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="preface-by-the-author.html"><i class="fa fa-check"></i><b>1</b> Preface by the Author</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>2.1</b> Story Time</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#lightning-never-strikes-twice"><i class="fa fa-check"></i>Lightning Never Strikes Twice</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#trust-fall"><i class="fa fa-check"></i>Trust Fall</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#fermis-paperclips"><i class="fa fa-check"></i>Fermi’s Paperclips</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2.2</b> What Is Machine Learning?</a></li>
<li class="chapter" data-level="2.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>2.3</b> Terminology</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a><ul>
<li class="chapter" data-level="3.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>3.1</b> Importance of Interpretability</a></li>
<li class="chapter" data-level="3.2" data-path="taxonomy-of-interpretability-methods.html"><a href="taxonomy-of-interpretability-methods.html"><i class="fa fa-check"></i><b>3.2</b> Taxonomy of Interpretability Methods</a></li>
<li class="chapter" data-level="3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html"><i class="fa fa-check"></i><b>3.3</b> Scope of Interpretability</a><ul>
<li class="chapter" data-level="3.3.1" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#algorithm-transparency"><i class="fa fa-check"></i><b>3.3.1</b> Algorithm Transparency</a></li>
<li class="chapter" data-level="3.3.2" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-holistic-model-interpretability"><i class="fa fa-check"></i><b>3.3.2</b> Global, Holistic Model Interpretability</a></li>
<li class="chapter" data-level="3.3.3" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#global-model-interpretability-on-a-modular-level"><i class="fa fa-check"></i><b>3.3.3</b> Global Model Interpretability on a Modular Level</a></li>
<li class="chapter" data-level="3.3.4" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-single-prediction"><i class="fa fa-check"></i><b>3.3.4</b> Local Interpretability for a Single Prediction</a></li>
<li class="chapter" data-level="3.3.5" data-path="scope-of-interpretability.html"><a href="scope-of-interpretability.html#local-interpretability-for-a-group-of-predictions"><i class="fa fa-check"></i><b>3.3.5</b> Local Interpretability for a Group of Predictions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="evaluation-of-interpretability.html"><a href="evaluation-of-interpretability.html"><i class="fa fa-check"></i><b>3.4</b> Evaluation of Interpretability</a></li>
<li class="chapter" data-level="3.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>3.5</b> Properties of Explanations</a></li>
<li class="chapter" data-level="3.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>3.6</b> Human-friendly Explanations</a><ul>
<li class="chapter" data-level="3.6.1" data-path="explanation.html"><a href="explanation.html#what-is-an-explanation"><i class="fa fa-check"></i><b>3.6.1</b> What Is an Explanation?</a></li>
<li class="chapter" data-level="3.6.2" data-path="explanation.html"><a href="explanation.html#good-explanation"><i class="fa fa-check"></i><b>3.6.2</b> What Is a Good Explanation?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Datasets</a><ul>
<li class="chapter" data-level="4.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>4.1</b> Bike Rentals (Regression)</a></li>
<li class="chapter" data-level="4.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>4.2</b> YouTube Spam Comments (Text Classification)</a></li>
<li class="chapter" data-level="4.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>4.3</b> Risk Factors for Cervical Cancer (Classification)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Interpretable Models</a><ul>
<li class="chapter" data-level="5.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>5.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>5.1.2</b> Example</a></li>
<li class="chapter" data-level="5.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>5.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="5.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>5.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="5.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>5.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="5.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>5.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="5.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>5.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="5.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>5.1.8</b> Advantages</a></li>
<li class="chapter" data-level="5.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>5.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic.html"><a href="logistic.html#what-is-wrong-with-linear-regression-for-classification"><i class="fa fa-check"></i><b>5.2.1</b> What is Wrong with Linear Regression for Classification?</a></li>
<li class="chapter" data-level="5.2.2" data-path="logistic.html"><a href="logistic.html#theory"><i class="fa fa-check"></i><b>5.2.2</b> Theory</a></li>
<li class="chapter" data-level="5.2.3" data-path="logistic.html"><a href="logistic.html#interpretation-1"><i class="fa fa-check"></i><b>5.2.3</b> Interpretation</a></li>
<li class="chapter" data-level="5.2.4" data-path="logistic.html"><a href="logistic.html#example-1"><i class="fa fa-check"></i><b>5.2.4</b> Example</a></li>
<li class="chapter" data-level="5.2.5" data-path="logistic.html"><a href="logistic.html#advantages-and-disadvantages"><i class="fa fa-check"></i><b>5.2.5</b> Advantages and Disadvantages</a></li>
<li class="chapter" data-level="5.2.6" data-path="logistic.html"><a href="logistic.html#software"><i class="fa fa-check"></i><b>5.2.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>5.3</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="5.3.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>5.3.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="5.3.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>5.3.2</b> Interactions</a></li>
<li class="chapter" data-level="5.3.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>5.3.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="5.3.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>5.3.4</b> Advantages</a></li>
<li class="chapter" data-level="5.3.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>5.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="extend-lm.html"><a href="extend-lm.html#software-1"><i class="fa fa-check"></i><b>5.3.6</b> Software</a></li>
<li class="chapter" data-level="5.3.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>5.3.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>5.4</b> Decision Tree</a><ul>
<li class="chapter" data-level="5.4.1" data-path="tree.html"><a href="tree.html#interpretation-2"><i class="fa fa-check"></i><b>5.4.1</b> Interpretation</a></li>
<li class="chapter" data-level="5.4.2" data-path="tree.html"><a href="tree.html#example-2"><i class="fa fa-check"></i><b>5.4.2</b> Example</a></li>
<li class="chapter" data-level="5.4.3" data-path="tree.html"><a href="tree.html#advantages-2"><i class="fa fa-check"></i><b>5.4.3</b> Advantages</a></li>
<li class="chapter" data-level="5.4.4" data-path="tree.html"><a href="tree.html#disadvantages-2"><i class="fa fa-check"></i><b>5.4.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="tree.html"><a href="tree.html#software-2"><i class="fa fa-check"></i><b>5.4.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>5.5</b> Decision Rules</a><ul>
<li class="chapter" data-level="5.5.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>5.5.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="5.5.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>5.5.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="5.5.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>5.5.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="5.5.4" data-path="rules.html"><a href="rules.html#advantages-3"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="rules.html"><a href="rules.html#disadvantages-3"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>5.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>5.6</b> RuleFit</a><ul>
<li class="chapter" data-level="5.6.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>5.6.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="5.6.2" data-path="rulefit.html"><a href="rulefit.html#theory-1"><i class="fa fa-check"></i><b>5.6.2</b> Theory</a></li>
<li class="chapter" data-level="5.6.3" data-path="rulefit.html"><a href="rulefit.html#advantages-4"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-4"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>5.6.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>5.7</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>5.7.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="5.7.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.7.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>6</b> Model-Agnostic Methods</a></li>
<li class="chapter" data-level="7" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>7</b> Example-Based Explanations</a></li>
<li class="chapter" data-level="8" data-path="global-methods.html"><a href="global-methods.html"><i class="fa fa-check"></i><b>8</b> Global Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="8.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>8.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="pdp.html"><a href="pdp.html#pdp-based-feature-importance"><i class="fa fa-check"></i><b>8.1.1</b> PDP-based Feature Importance</a></li>
<li class="chapter" data-level="8.1.2" data-path="pdp.html"><a href="pdp.html#examples"><i class="fa fa-check"></i><b>8.1.2</b> Examples</a></li>
<li class="chapter" data-level="8.1.3" data-path="pdp.html"><a href="pdp.html#advantages-5"><i class="fa fa-check"></i><b>8.1.3</b> Advantages</a></li>
<li class="chapter" data-level="8.1.4" data-path="pdp.html"><a href="pdp.html#disadvantages-5"><i class="fa fa-check"></i><b>8.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.1.5" data-path="pdp.html"><a href="pdp.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>8.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>8.2</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="8.2.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>8.2.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="8.2.2" data-path="ale.html"><a href="ale.html#theory-2"><i class="fa fa-check"></i><b>8.2.2</b> Theory</a></li>
<li class="chapter" data-level="8.2.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>8.2.3</b> Estimation</a></li>
<li class="chapter" data-level="8.2.4" data-path="ale.html"><a href="ale.html#examples-1"><i class="fa fa-check"></i><b>8.2.4</b> Examples</a></li>
<li class="chapter" data-level="8.2.5" data-path="ale.html"><a href="ale.html#advantages-6"><i class="fa fa-check"></i><b>8.2.5</b> Advantages</a></li>
<li class="chapter" data-level="8.2.6" data-path="ale.html"><a href="ale.html#disadvantages-6"><i class="fa fa-check"></i><b>8.2.6</b> Disadvantages</a></li>
<li class="chapter" data-level="8.2.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>8.2.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>8.3</b> Feature Interaction</a><ul>
<li class="chapter" data-level="8.3.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>8.3.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="8.3.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>8.3.2</b> Theory: Friedman’s H-statistic</a></li>
<li class="chapter" data-level="8.3.3" data-path="interaction.html"><a href="interaction.html#examples-2"><i class="fa fa-check"></i><b>8.3.3</b> Examples</a></li>
<li class="chapter" data-level="8.3.4" data-path="interaction.html"><a href="interaction.html#advantages-7"><i class="fa fa-check"></i><b>8.3.4</b> Advantages</a></li>
<li class="chapter" data-level="8.3.5" data-path="interaction.html"><a href="interaction.html#disadvantages-7"><i class="fa fa-check"></i><b>8.3.5</b> Disadvantages</a></li>
<li class="chapter" data-level="8.3.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>8.3.6</b> Implementations</a></li>
<li class="chapter" data-level="8.3.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>8.3.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="decomposition.html"><a href="decomposition.html"><i class="fa fa-check"></i><b>8.4</b> Functional Decompositon</a><ul>
<li class="chapter" data-level="8.4.1" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-i"><i class="fa fa-check"></i><b>8.4.1</b> How not to Compute the Components I</a></li>
<li class="chapter" data-level="8.4.2" data-path="decomposition.html"><a href="decomposition.html#functional-decomposition"><i class="fa fa-check"></i><b>8.4.2</b> Functional Decomposition</a></li>
<li class="chapter" data-level="8.4.3" data-path="decomposition.html"><a href="decomposition.html#how-not-to-compute-the-components-ii"><i class="fa fa-check"></i><b>8.4.3</b> How not to Compute the Components II</a></li>
<li class="chapter" data-level="8.4.4" data-path="decomposition.html"><a href="decomposition.html#functional-anova"><i class="fa fa-check"></i><b>8.4.4</b> Functional ANOVA</a></li>
<li class="chapter" data-level="8.4.5" data-path="decomposition.html"><a href="decomposition.html#generalized-functional-anova-for-dependent-features"><i class="fa fa-check"></i><b>8.4.5</b> Generalized Functional ANOVA for Dependent Features</a></li>
<li class="chapter" data-level="8.4.6" data-path="decomposition.html"><a href="decomposition.html#accumulated-local-effect-plots"><i class="fa fa-check"></i><b>8.4.6</b> Accumulated Local Effect Plots</a></li>
<li class="chapter" data-level="8.4.7" data-path="decomposition.html"><a href="decomposition.html#statistical-regression-models"><i class="fa fa-check"></i><b>8.4.7</b> Statistical Regression Models</a></li>
<li class="chapter" data-level="8.4.8" data-path="decomposition.html"><a href="decomposition.html#bonus-partial-dependence-plot"><i class="fa fa-check"></i><b>8.4.8</b> Bonus: Partial Dependence Plot</a></li>
<li class="chapter" data-level="8.4.9" data-path="decomposition.html"><a href="decomposition.html#advantages-8"><i class="fa fa-check"></i><b>8.4.9</b> Advantages</a></li>
<li class="chapter" data-level="8.4.10" data-path="decomposition.html"><a href="decomposition.html#disadvantages-8"><i class="fa fa-check"></i><b>8.4.10</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>8.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="8.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-3"><i class="fa fa-check"></i><b>8.5.1</b> Theory</a></li>
<li class="chapter" data-level="8.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>8.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="8.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>8.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="8.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-9"><i class="fa fa-check"></i><b>8.5.4</b> Advantages</a></li>
<li class="chapter" data-level="8.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-9"><i class="fa fa-check"></i><b>8.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="8.5.6" data-path="feature-importance.html"><a href="feature-importance.html#alternatives-1"><i class="fa fa-check"></i><b>8.5.6</b> Alternatives</a></li>
<li class="chapter" data-level="8.5.7" data-path="feature-importance.html"><a href="feature-importance.html#software-3"><i class="fa fa-check"></i><b>8.5.7</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>8.6</b> Global Surrogate</a><ul>
<li class="chapter" data-level="8.6.1" data-path="global.html"><a href="global.html#theory-4"><i class="fa fa-check"></i><b>8.6.1</b> Theory</a></li>
<li class="chapter" data-level="8.6.2" data-path="global.html"><a href="global.html#example-3"><i class="fa fa-check"></i><b>8.6.2</b> Example</a></li>
<li class="chapter" data-level="8.6.3" data-path="global.html"><a href="global.html#advantages-10"><i class="fa fa-check"></i><b>8.6.3</b> Advantages</a></li>
<li class="chapter" data-level="8.6.4" data-path="global.html"><a href="global.html#disadvantages-10"><i class="fa fa-check"></i><b>8.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.6.5" data-path="global.html"><a href="global.html#software-4"><i class="fa fa-check"></i><b>8.6.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>8.7</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="8.7.1" data-path="proto.html"><a href="proto.html#theory-5"><i class="fa fa-check"></i><b>8.7.1</b> Theory</a></li>
<li class="chapter" data-level="8.7.2" data-path="proto.html"><a href="proto.html#examples-3"><i class="fa fa-check"></i><b>8.7.2</b> Examples</a></li>
<li class="chapter" data-level="8.7.3" data-path="proto.html"><a href="proto.html#advantages-11"><i class="fa fa-check"></i><b>8.7.3</b> Advantages</a></li>
<li class="chapter" data-level="8.7.4" data-path="proto.html"><a href="proto.html#disadvantages-11"><i class="fa fa-check"></i><b>8.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="8.7.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>8.7.5</b> Code and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-methods.html"><a href="local-methods.html"><i class="fa fa-check"></i><b>9</b> Local Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>9.1</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ice.html"><a href="ice.html#examples-4"><i class="fa fa-check"></i><b>9.1.1</b> Examples</a></li>
<li class="chapter" data-level="9.1.2" data-path="ice.html"><a href="ice.html#advantages-12"><i class="fa fa-check"></i><b>9.1.2</b> Advantages</a></li>
<li class="chapter" data-level="9.1.3" data-path="ice.html"><a href="ice.html#disadvantages-12"><i class="fa fa-check"></i><b>9.1.3</b> Disadvantages</a></li>
<li class="chapter" data-level="9.1.4" data-path="ice.html"><a href="ice.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>9.1.4</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>9.2</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>9.2.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>9.2.2</b> LIME for Text</a></li>
<li class="chapter" data-level="9.2.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>9.2.3</b> LIME for Images</a></li>
<li class="chapter" data-level="9.2.4" data-path="lime.html"><a href="lime.html#advantages-13"><i class="fa fa-check"></i><b>9.2.4</b> Advantages</a></li>
<li class="chapter" data-level="9.2.5" data-path="lime.html"><a href="lime.html#disadvantages-13"><i class="fa fa-check"></i><b>9.2.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>9.3</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="9.3.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>9.3.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="9.3.2" data-path="counterfactual.html"><a href="counterfactual.html#example-8"><i class="fa fa-check"></i><b>9.3.2</b> Example</a></li>
<li class="chapter" data-level="9.3.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-14"><i class="fa fa-check"></i><b>9.3.3</b> Advantages</a></li>
<li class="chapter" data-level="9.3.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-14"><i class="fa fa-check"></i><b>9.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="9.3.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>9.3.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>9.4</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="9.4.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>9.4.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="9.4.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>9.4.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="9.4.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>9.4.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="9.4.4" data-path="anchors.html"><a href="anchors.html#advantages-15"><i class="fa fa-check"></i><b>9.4.4</b> Advantages</a></li>
<li class="chapter" data-level="9.4.5" data-path="anchors.html"><a href="anchors.html#disadvantages-15"><i class="fa fa-check"></i><b>9.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="9.4.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>9.4.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>9.5</b> Shapley Values</a><ul>
<li class="chapter" data-level="9.5.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>9.5.1</b> General Idea</a></li>
<li class="chapter" data-level="9.5.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>9.5.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="9.5.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>9.5.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="9.5.4" data-path="shapley.html"><a href="shapley.html#advantages-16"><i class="fa fa-check"></i><b>9.5.4</b> Advantages</a></li>
<li class="chapter" data-level="9.5.5" data-path="shapley.html"><a href="shapley.html#disadvantages-16"><i class="fa fa-check"></i><b>9.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="9.5.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>9.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>9.6</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="9.6.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>9.6.1</b> Definition</a></li>
<li class="chapter" data-level="9.6.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>9.6.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="9.6.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>9.6.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="9.6.4" data-path="shap.html"><a href="shap.html#examples-5"><i class="fa fa-check"></i><b>9.6.4</b> Examples</a></li>
<li class="chapter" data-level="9.6.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>9.6.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="9.6.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>9.6.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="9.6.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>9.6.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="9.6.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>9.6.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="9.6.9" data-path="shap.html"><a href="shap.html#clustering-shapley-values"><i class="fa fa-check"></i><b>9.6.9</b> Clustering Shapley Values</a></li>
<li class="chapter" data-level="9.6.10" data-path="shap.html"><a href="shap.html#advantages-17"><i class="fa fa-check"></i><b>9.6.10</b> Advantages</a></li>
<li class="chapter" data-level="9.6.11" data-path="shap.html"><a href="shap.html#disadvantages-17"><i class="fa fa-check"></i><b>9.6.11</b> Disadvantages</a></li>
<li class="chapter" data-level="9.6.12" data-path="shap.html"><a href="shap.html#software-5"><i class="fa fa-check"></i><b>9.6.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>10</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="10.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>10.1</b> Learned Features</a><ul>
<li class="chapter" data-level="10.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>10.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="10.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>10.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="10.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-18"><i class="fa fa-check"></i><b>10.1.3</b> Advantages</a></li>
<li class="chapter" data-level="10.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-18"><i class="fa fa-check"></i><b>10.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="10.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>10.1.5</b> Software and Further Material</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html"><i class="fa fa-check"></i><b>10.2</b> Pixel Attribution (Saliency Maps)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="pixel-attribution.html"><a href="pixel-attribution.html#vanilla-gradient-saliency-maps"><i class="fa fa-check"></i><b>10.2.1</b> Vanilla Gradient (Saliency Maps)</a></li>
<li class="chapter" data-level="10.2.2" data-path="pixel-attribution.html"><a href="pixel-attribution.html#deconvnet"><i class="fa fa-check"></i><b>10.2.2</b> DeconvNet</a></li>
<li class="chapter" data-level="10.2.3" data-path="pixel-attribution.html"><a href="pixel-attribution.html#grad-cam"><i class="fa fa-check"></i><b>10.2.3</b> Grad-CAM</a></li>
<li class="chapter" data-level="10.2.4" data-path="pixel-attribution.html"><a href="pixel-attribution.html#guided-grad-cam"><i class="fa fa-check"></i><b>10.2.4</b> Guided Grad-CAM</a></li>
<li class="chapter" data-level="10.2.5" data-path="pixel-attribution.html"><a href="pixel-attribution.html#smoothgrad"><i class="fa fa-check"></i><b>10.2.5</b> SmoothGrad</a></li>
<li class="chapter" data-level="10.2.6" data-path="pixel-attribution.html"><a href="pixel-attribution.html#examples-6"><i class="fa fa-check"></i><b>10.2.6</b> Examples</a></li>
<li class="chapter" data-level="10.2.7" data-path="pixel-attribution.html"><a href="pixel-attribution.html#advantages-19"><i class="fa fa-check"></i><b>10.2.7</b> Advantages</a></li>
<li class="chapter" data-level="10.2.8" data-path="pixel-attribution.html"><a href="pixel-attribution.html#disadvantages-19"><i class="fa fa-check"></i><b>10.2.8</b> Disadvantages</a></li>
<li class="chapter" data-level="10.2.9" data-path="pixel-attribution.html"><a href="pixel-attribution.html#software-6"><i class="fa fa-check"></i><b>10.2.9</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html"><i class="fa fa-check"></i><b>10.3</b> Detecting Concepts</a><ul>
<li class="chapter" data-level="10.3.1" data-path="detecting-concepts.html"><a href="detecting-concepts.html#tcav-testing-with-concept-activation-vectors"><i class="fa fa-check"></i><b>10.3.1</b> TCAV: Testing with Concept Activation Vectors</a></li>
<li class="chapter" data-level="10.3.2" data-path="detecting-concepts.html"><a href="detecting-concepts.html#example-9"><i class="fa fa-check"></i><b>10.3.2</b> Example</a></li>
<li class="chapter" data-level="10.3.3" data-path="detecting-concepts.html"><a href="detecting-concepts.html#advantages-20"><i class="fa fa-check"></i><b>10.3.3</b> Advantages</a></li>
<li class="chapter" data-level="10.3.4" data-path="detecting-concepts.html"><a href="detecting-concepts.html#disadvantages-20"><i class="fa fa-check"></i><b>10.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="10.3.5" data-path="detecting-concepts.html"><a href="detecting-concepts.html#bonus-other-concept-based-approaches"><i class="fa fa-check"></i><b>10.3.5</b> Bonus: Other Concept-based Approaches</a></li>
<li class="chapter" data-level="10.3.6" data-path="detecting-concepts.html"><a href="detecting-concepts.html#software-7"><i class="fa fa-check"></i><b>10.3.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>10.4</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="10.4.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>10.4.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="10.4.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>10.4.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>10.5</b> Influential Instances</a><ul>
<li class="chapter" data-level="10.5.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>10.5.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="10.5.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>10.5.2</b> Influence Functions</a></li>
<li class="chapter" data-level="10.5.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="10.5.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>10.5.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="10.5.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-5"><i class="fa fa-check"></i><b>10.5.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>11</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="11.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>11.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="11.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>11.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>12</b> Contribute to the Book</a></li>
<li class="chapter" data-level="13" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>13</b> Citing this Book</a></li>
<li class="chapter" data-level="14" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>14</b> Translations</a></li>
<li class="chapter" data-level="15" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>15</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used.html"><a href="r-packages-used.html"><i class="fa fa-check"></i>R Packages Used</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shap" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.6</span> SHAP (SHapley Additive exPlanations)<a href="shap.html#shap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>SHAP (SHapley Additive exPlanations) by Lundberg and Lee (2017)<a href="#fn69" class="footnote-ref" id="fnref69"><sup>69</sup></a> is a method to explain individual predictions.
SHAP is based on the game theoretically optimal <a href="shapley.html#shapley">Shapley values</a>.</p>
<p>There are two reasons why SHAP got its own chapter and is not a subchapter of <a href="shapley.html#shapley">Shapley values</a>.
First, the SHAP authors proposed KernelSHAP, an alternative, kernel-based estimation approach for Shapley values inspired by <a href="lime.html#lime">local surrogate models</a>.
And they proposed TreeSHAP, an efficient estimation approach for tree-based models.
Second, SHAP comes with many global interpretation methods based on aggregations of Shapley values.
This chapter explains both the new estimation approaches and the global interpretation methods.</p>

<div class="rmdnote">
Interested in an in-depth, hands-on course on SHAP and Shapley values?
Head over to <a href="https://leanpub.com/c/shapley-xai">the Shapley course page</a> and get notified once the course is available.
</div>

<p>I recommend reading the chapters on <a href="shapley.html#shapley">Shapley values</a> and <a href="lime.html#lime">local models (LIME)</a> first.</p>
<div id="definition" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.1</span> Definition<a href="shap.html#definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The goal of SHAP is to explain the prediction of an instance x by computing the contribution of each feature to the prediction.
The SHAP explanation method computes Shapley values from coalitional game theory.
The feature values of a data instance act as players in a coalition.
Shapley values tell us how to fairly distribute the “payout” (= the prediction) among the features.
A player can be an individual feature value, e.g. for tabular data.
A player can also be a group of feature values.
For example to explain an image, pixels can be grouped to superpixels and the prediction distributed among them.
One innovation that SHAP brings to the table is that the Shapley value explanation is represented as an additive feature attribution method, a linear model.
That view connects LIME and Shapley values.
SHAP specifies the explanation as:</p>
<p><span class="math display">\[g(z&#39;)=\phi_0+\sum_{j=1}^M\phi_jz_j&#39;\]</span></p>
<p>where g is the explanation model, <span class="math inline">\(z&#39;\in\{0,1\}^M\)</span> is the coalition vector, M is the maximum coalition size and <span class="math inline">\(\phi_j\in\mathbb{R}\)</span> is the feature attribution for a feature j, the Shapley values.
What I call “coalition vector” is called “simplified features” in the SHAP paper.
I think this name was chosen, because for e.g. image data, the images are not represented on the pixel level, but aggregated to superpixels.
I believe it is helpful to think about the z’s as describing coalitions:
In the coalition vector, an entry of 1 means that the corresponding feature value is “present” and 0 that it is “absent”.
This should sound familiar to you if you know about Shapley values.
To compute Shapley values, we simulate that only some feature values are playing (“present”) and some are not (“absent”).
The representation as a linear model of coalitions is a trick for the computation of the <span class="math inline">\(\phi\)</span>’s.
For x, the instance of interest, the coalition vector x’ is a vector of all 1’s, i.e. all feature values are “present”.
The formula simplifies to:</p>
<p><span class="math display">\[g(x&#39;)=\phi_0+\sum_{j=1}^M\phi_j\]</span></p>
<p>You can find this formula in similar notation in the <a href="shapley.html#shapley">Shapley value</a> chapter.
More about the actual estimation comes later.
Let us first talk about the properties of the <span class="math inline">\(\phi\)</span>’s before we go into the details of their estimation.</p>
<!-- Desirable properties -->
<p>Shapley values are the only solution that satisfies properties of Efficiency, Symmetry, Dummy and Additivity.
SHAP also satisfies these, since it computes Shapley values.
In the SHAP paper, you will find discrepancies between SHAP properties and Shapley properties.
SHAP describes the following three desirable properties:</p>
<p><strong>1) Local accuracy</strong></p>
<p><span class="math display">\[\hat{f}(x)=g(x&#39;)=\phi_0+\sum_{j=1}^M\phi_jx_j&#39;\]</span></p>
<p>If you define <span class="math inline">\(\phi_0=E_X(\hat{f}(x))\)</span> and set all <span class="math inline">\(x_j&#39;\)</span> to 1, this is the Shapley efficiency property.
Only with a different name and using the coalition vector.</p>
<p><span class="math display">\[\hat{f}(x)=\phi_0+\sum_{j=1}^M\phi_jx_j&#39;=E_X(\hat{f}(X))+\sum_{j=1}^M\phi_j\]</span></p>
<p><strong>2) Missingness</strong></p>
<p><span class="math display">\[x_j&#39;=0\Rightarrow\phi_j=0\]</span></p>
<p>Missingness says that a missing feature gets an attribution of zero.
Note that <span class="math inline">\(x_j&#39;\)</span> refers to the coalitions where a value of 0 represents the absence of a feature value.
In coalition notation, all feature values <span class="math inline">\(x_j&#39;\)</span> of the instance to be explained should be ‘1’.
The presence of a 0 would mean that the feature value is missing for the instance of interest.
This property is not among the properties of the “normal” Shapley values.
So why do we need it for SHAP?
Lundberg calls it a <a href="https://github.com/slundberg/shap/issues/175#issuecomment-407134438">“minor book-keeping property”</a>.
A missing feature could – in theory – have an arbitrary Shapley value without hurting the local accuracy property, since it is multiplied with <span class="math inline">\(x_j&#39;=0\)</span>.
The Missingness property enforces that missing features get a Shapley value of 0.
In practice, this is only relevant for features that are constant.</p>
<p><strong>3) Consistency</strong></p>
<p>Let <span class="math inline">\(\hat{f}_x(z&#39;)=\hat{f}(h_x(z&#39;))\)</span> and <span class="math inline">\(z_{\setminus{}j}&#39;\)</span> indicate that <span class="math inline">\(z_j&#39;=0\)</span>.
For any two models f and f’ that satisfy:</p>
<p><span class="math display">\[\hat{f}_x&#39;(z&#39;)-\hat{f}_x&#39;(z_{\setminus{}j}&#39;)\geq{}\hat{f}_x(z&#39;)-\hat{f}_x(z_{\setminus{}j}&#39;)\]</span></p>
<p>for all inputs <span class="math inline">\(z&#39;\in\{0,1\}^M\)</span>, then:</p>
<p><span class="math display">\[\phi_j(\hat{f}&#39;,x)\geq\phi_j(\hat{f},x)\]</span></p>
<p>The consistency property says that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.
From Consistency the Shapley properties Linearity, Dummy and Symmetry follow, as described in the Appendix of Lundberg and Lee.</p>
</div>
<div id="kernelshap" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.2</span> KernelSHAP<a href="shap.html#kernelshap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- The general Idea of linear model -->
<p>KernelSHAP estimates for an instance x the contributions of each feature value to the prediction.
KernelSHAP consists of five steps:</p>
<ul>
<li>Sample coalitions <span class="math inline">\(z_k&#39;\in\{0,1\}^M,\quad{}k\in\{1,\ldots,K\}\)</span> (1 = feature present in coalition, 0 = feature absent).</li>
<li>Get prediction for each <span class="math inline">\(z_k&#39;\)</span> by first converting <span class="math inline">\(z_k&#39;\)</span> to the original feature space and then applying model <span class="math inline">\(\hat{f}: \hat{f}(h_x(z_k&#39;))\)</span></li>
<li>Compute the weight for each <span class="math inline">\(z_k&#39;\)</span> with the SHAP kernel.</li>
<li>Fit weighted linear model.</li>
<li>Return Shapley values <span class="math inline">\(\phi_k\)</span>, the coefficients from the linear model.</li>
</ul>
<p>We can create a random coalition by repeated coin flips until we have a chain of 0’s and 1’s.
For example, the vector of (1,0,1,0) means that we have a coalition of the first and third features.
The K sampled coalitions become the dataset for the regression model.
The target for the regression model is the prediction for a coalition.
(“Hold on!,” you say. “The model has not been trained on these binary coalition data and cannot make predictions for them.”)
To get from coalitions of feature values to valid data instances, we need a function <span class="math inline">\(h_x(z&#39;)=z\)</span> where <span class="math inline">\(h_x:\{0,1\}^M\rightarrow\mathbb{R}^p\)</span>.
The function <span class="math inline">\(h_x\)</span> maps 1’s to the corresponding value from the instance x that we want to explain.
For tabular data, it maps 0’s to the values of another instance that we sample from the data.
This means that we equate “feature value is absent” with “feature value is replaced by random feature value from data”.
For tabular data, the following figure visualizes the mapping from coalitions to feature values:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:shap-simplified-feature"></span>
<img src="images/shap-simplified-features.jpg" alt="Function $h_x$ maps a coalition to a valid instance. For present features (1), $h_x$ maps to the feature values of x. For absent features (0), $h_x$ maps to the values of a randomly sampled data instance." width="\textwidth" />
<p class="caption">
FIGURE 9.22: Function <span class="math inline">\(h_x\)</span> maps a coalition to a valid instance. For present features (1), <span class="math inline">\(h_x\)</span> maps to the feature values of x. For absent features (0), <span class="math inline">\(h_x\)</span> maps to the values of a randomly sampled data instance.
</p>
</div>
<p><span class="math inline">\(h_x\)</span> for tabular data treats <span class="math inline">\(X_C\)</span> and <span class="math inline">\(X_S\)</span> as independent and integrates over the marginal distribution:</p>
<p><span class="math display">\[\hat{f}(h_x(z&#39;))=E_{X_C}[\hat{f}(x)]\]</span></p>
<p>Sampling from the marginal distribution means ignoring the dependence structure between present and absent features.
KernelSHAP therefore suffers from the same problem as all permutation-based interpretation methods.
The estimation puts too much weight on unlikely instances.
Results can become unreliable.
But it is necessary to sample from the marginal distribution.
The solution would be to sample from the conditional distribution, which changes the value function, and therefore the game to which Shapley values are the solution.
As a result, the Shapley values have a different interpretation:
For example, a feature that might not have been used by the model at all can have a non-zero Shapley value when the conditional sampling is used.
For the marginal game, this feature value would always get a Shapley value of 0, because otherwise it would violate the Dummy axiom.</p>
<p>For images, the following figure describes a possible mapping function:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-43"></span>
<img src="images/shap-superpixel.jpg" alt="Function $h_x$ maps coalitions of superpixels (sp) to images. Superpixels are groups of pixels. For present features (1), $h_x$ returns the corresponding part of the original image. For absent features (0), $h_x$ greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option." width="\textwidth" />
<p class="caption">
FIGURE 9.23: Function <span class="math inline">\(h_x\)</span> maps coalitions of superpixels (sp) to images. Superpixels are groups of pixels. For present features (1), <span class="math inline">\(h_x\)</span> returns the corresponding part of the original image. For absent features (0), <span class="math inline">\(h_x\)</span> greys out the corresponding area. Assigning the average color of surrounding pixels or similar would also be an option.
</p>
</div>
<!-- Kernel -->
<p>The big difference to LIME is the weighting of the instances in the regression model.
LIME weights the instances according to how close they are to the original instance.
The more 0’s in the coalition vector, the smaller the weight in LIME.
SHAP weights the sampled instances according to the weight the coalition would get in the Shapley value estimation.
Small coalitions (few 1’s) and large coalitions (i.e. many 1’s) get the largest weights.
The intuition behind it is:
We learn most about individual features if we can study their effects in isolation.
If a coalition consists of a single feature, we can learn about this feature’s isolated main effect on the prediction.
If a coalition consists of all but one feature, we can learn about this feature’s total effect (main effect plus feature interactions).
If a coalition consists of half the features, we learn little about an individual feature’s contribution, as there are many possible coalitions with half of the features.
To achieve Shapley compliant weighting, Lundberg et al. propose the SHAP kernel:</p>
<p><span class="math display">\[\pi_{x}(z&#39;)=\frac{(M-1)}{\binom{M}{|z&#39;|}|z&#39;|(M-|z&#39;|)}\]</span></p>
<p>Here, M is the maximum coalition size and <span class="math inline">\(|z&#39;|\)</span> the number of present features in instance z’.
Lundberg and Lee show that linear regression with this kernel weight yields Shapley values.
If you would use the SHAP kernel with LIME on the coalition data, LIME would also estimate Shapley values!</p>
<!-- Sampling trick -->
<p>We can be a bit smarter about the sampling of coalitions:
The smallest and largest coalitions take up most of the weight.
We get better Shapley value estimates by using some of the sampling budget K to include these high-weight coalitions instead of sampling blindly.
We start with all possible coalitions with 1 and M-1 features, which makes 2 times M coalitions in total.
When we have enough budget left (current budget is K - 2M), we can include coalitions with 2 features and with M-2 features and so on.
From the remaining coalition sizes, we sample with readjusted weights.</p>
<!-- Linear Model -->
<p>We have the data, the target and the weights;
Everything we need to build our weighted linear regression model:</p>
<p><span class="math display">\[g(z&#39;)=\phi_0+\sum_{j=1}^M\phi_jz_j&#39;\]</span></p>
<p>We train the linear model g by optimizing the following loss function L:</p>
<p><span class="math display">\[L(\hat{f},g,\pi_{x})=\sum_{z&#39;\in{}Z}[\hat{f}(h_x(z&#39;))-g(z&#39;)]^2\pi_{x}(z&#39;)\]</span></p>
<p>where Z is the training data.
This is the good old boring sum of squared errors that we usually optimize for linear models.
The estimated coefficients of the model, the <span class="math inline">\(\phi_j\)</span>’s, are the Shapley values.</p>
<p>Since we are in a linear regression setting, we can also make use of the standard tools for regression.
For example, we can add regularization terms to make the model sparse.
If we add an L1 penalty to the loss L, we can create sparse explanations.
(I am not so sure whether the resulting coefficients would still be valid Shapley values though.)</p>
</div>
<div id="treeshap" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.3</span> TreeSHAP<a href="shap.html#treeshap" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Lundberg et al. (2018)[^tree-shap] proposed TreeSHAP, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees.
TreeSHAP was introduced as a fast, model-specific alternative to KernelSHAP, but it turned out that it can produce unintuitive feature attributions.</p>
<p>TreeSHAP defines the value function using the conditional expectation <span class="math inline">\(E_{X_S|X_C}(\hat{f}(x)|x_S)\)</span> instead of the marginal expectation.
The problem with the conditional expectation is that features that have no influence on the prediction function f can get a TreeSHAP estimate different from zero as shown by Sundararajan et al. (2019) <a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a> and Janzing et al. (2019) <a href="#fn71" class="footnote-ref" id="fnref71"><sup>71</sup></a>.
The non-zero estimate can happen when the feature is correlated with another feature that actually has an influence on the prediction.</p>
<p>How much faster is TreeSHAP?
Compared to exact KernelSHAP, it reduces the computational complexity from <span class="math inline">\(O(TL2^M)\)</span> to <span class="math inline">\(O(TLD^2)\)</span>, where T is the number of trees, L is the maximum number of leaves in any tree and D the maximal depth of any tree.</p>
<!-- To explain an individual prediction with exact Shapley values, we have to estimate  $E(\hat{f}(x)|x_S)$ for all possible feature value subsets S.-->
<p>TreeSHAP uses the conditional expectation <span class="math inline">\(E_{X_S|X_C}(\hat{f}(x)|x_S)\)</span> to estimate effects.
I will give you some intuition on how we can compute the expected prediction for a single tree, an instance x and feature subset S.
If we conditioned on all features – if S was the set of all features – then the prediction from the node in which the instance x falls would be the expected prediction.
If we would not condition the prediction on any feature – if S was empty – we would use the weighted average of predictions of all terminal nodes.
If S contains some, but not all, features, we ignore predictions of unreachable nodes.
Unreachable means that the decision path that leads to this node contradicts values in <span class="math inline">\(x_S\)</span>.
From the remaining terminal nodes, we average the predictions weighted by node sizes (i.e. number of training samples in that node).
The mean of the remaining terminal nodes, weighted by the number of instances per node, is the expected prediction for x given S.
The problem is that we have to apply this procedure for each possible subset S of the feature values.
<!--
This means $\sum_{i=1}{p}\frac{(p-i)!i!}{i!}$ times.
Here, each summand is the set of all possible subsets S with the same cardinality (e.g. all possible subsets with 2 features).
-->
TreeSHAP computes in polynomial time instead of exponential.
The basic idea is to push all possible subsets S down the tree at the same time.
For each decision node we have to keep track of the number of subsets.
This depends on the subsets in the parent node and the split feature.
For example, when the first split in a tree is on feature x3, then all the subsets that contain feature x3 will go to one node (the one where x goes).
Subsets that do not contain feature x3 go to both nodes with reduced weight.
Unfortunately, subsets of different sizes have different weights.
The algorithm has to keep track of the overall weight of the subsets in each node.
This complicates the algorithm.
I refer to the original paper for details of TreeSHAP.
The computation can be expanded to more trees:
Thanks to the Additivity property of Shapley values, the Shapley values of a tree ensemble is the (weighted) average of the Shapley values of the individual trees.</p>
<p>Next, we will look at SHAP explanations in action.</p>
</div>
<div id="examples-5" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.4</span> Examples<a href="shap.html#examples-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I trained a random forest classifier with 100 trees to predict the <a href="cervical.html#cervical">risk for cervical cancer</a>.
We will use SHAP to explain individual predictions.
We can use the fast TreeSHAP estimation method instead of the slower KernelSHAP method, since a random forest is an ensemble of trees.
But instead of relying on the conditional distribution, this example uses the marginal distribution.
This is described in the package, but not in the original paper.
The Python TreeSHAP function is slower with the marginal distribution, but still faster than KernelSHAP, since it scales linearly with the rows in the data.</p>
<p>Because we use the marginal distribution here, the interpretation is the same as in the <a href="shapley.html#shapley">Shapley value chapter</a>.
But with the Python shap package comes a different visualization:
You can visualize feature attributions such as Shapley values as “forces”.
Each feature value is a force that either increases or decreases the prediction.
The prediction starts from the baseline.
The baseline for Shapley values is the average of all predictions.
In the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction.
These forces balance each other out at the actual prediction of the data instance.</p>
<p>The following figure shows SHAP explanation force plots for two women from the cervical cancer dataset:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-44"></span>
<img src="images/unnamed-chunk-44-1.png" alt="SHAP values to explain the predicted cancer probabilities of two individuals. The baseline -- the average predicted probability -- is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk." width="\textwidth" />
<p class="caption">
FIGURE 9.24: SHAP values to explain the predicted cancer probabilities of two individuals. The baseline – the average predicted probability – is 0.066. The first woman has a low predicted risk of 0.06. Risk increasing effects such as STDs are offset by decreasing effects such as age. The second woman has a high predicted risk of 0.71. Age of 51 and 34 years of smoking increase her predicted cancer risk.
</p>
</div>
<p>These were explanations for individual predictions.</p>
<p>Shapley values can be combined into global explanations.
If we run SHAP for every instance, we get a matrix of Shapley values.
This matrix has one row per data instance and one column per feature.
We can interpret the entire model by analyzing the Shapley values in this matrix.</p>
<p>We start with SHAP feature importance.</p>
</div>
<div id="shap-feature-importance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.5</span> SHAP Feature Importance<a href="shap.html#shap-feature-importance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea behind SHAP feature importance is simple:
Features with large absolute Shapley values are important.
Since we want the global importance, we average the <strong>absolute</strong> Shapley values per feature across the data:</p>
<p><span class="math display">\[I_j=\frac{1}{n}\sum_{i=1}^n{}|\phi_j^{(i)}|\]</span></p>
<p>Next, we sort the features by decreasing importance and plot them.
The following figure shows the SHAP feature importance for the random forest trained before for predicting cervical cancer.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-45"></span>
<img src="images/shap-importance.png" alt="SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis)." width="\textwidth" />
<p class="caption">
FIGURE 9.25: SHAP feature importance measured as the mean absolute Shapley values. The number of years with hormonal contraceptives was the most important feature, changing the predicted absolute cancer probability on average by 2.4 percentage points (0.024 on x-axis).
</p>
</div>
<p>SHAP feature importance is an alternative to <a href="feature-importance.html#feature-importance">permutation feature importance</a>.
There is a big difference between both importance measures:
Permutation feature importance is based on the decrease in model performance.
SHAP is based on magnitude of feature attributions.</p>
<p>The feature importance plot is useful, but contains no information beyond the importances.
For a more informative plot, we will next look at the summary plot.</p>
</div>
<div id="shap-summary-plot" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.6</span> SHAP Summary Plot<a href="shap.html#shap-summary-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The summary plot combines feature importance with feature effects.
Each point on the summary plot is a Shapley value for a feature and an instance.
The position on the y-axis is determined by the feature and on the x-axis by the Shapley value.
The color represents the value of the feature from low to high.
Overlapping points are jittered in y-axis direction, so we get a sense of the distribution of the Shapley values per feature.
The features are ordered according to their importance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-46"></span>
<img src="images/shap-importance-extended.png" alt="SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world." width="\textwidth" />
<p class="caption">
FIGURE 9.26: SHAP summary plot. Low number of years on hormonal contraceptives reduce the predicted cancer risk, a large number of years increases the risk. Your regular reminder: All effects describe the behavior of the model and are not necessarily causal in the real world.
</p>
</div>
<p>In the summary plot, we see first indications of the relationship between the value of a feature and the impact on the prediction.
But to see the exact form of the relationship, we have to look at SHAP dependence plots.</p>
</div>
<div id="shap-dependence-plot" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.7</span> SHAP Dependence Plot<a href="shap.html#shap-dependence-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>SHAP feature dependence might be the simplest global interpretation plot:
1) Pick a feature.
2) For each data instance, plot a point with the feature value on the x-axis and the corresponding Shapley value on the y-axis.
3) Done.</p>
<p>Mathematically, the plot contains the following points: <span class="math inline">\(\{(x_j^{(i)},\phi_j^{(i)})\}_{i=1}^n\)</span></p>
<p>The following figure shows the SHAP feature dependence for years on hormonal contraceptives:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-47"></span>
<img src="images/shap-dependence.png" alt="SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability." width="\textwidth" />
<p class="caption">
FIGURE 9.27: SHAP dependence plot for years on hormonal contraceptives. Compared to 0 years, a few years lower the predicted probability and a high number of years increases the predicted cancer probability.
</p>
</div>
<p>SHAP dependence plots are an alternative to <a href="pdp.html#pdp">partial dependence plots</a> and <a href="ale.html#ale">accumulated local effects</a>.
While PDP and ALE plot show average effects, SHAP dependence also shows the variance on the y-axis.
Especially in case of interactions, the SHAP dependence plot will be much more dispersed in the y-axis.
The dependence plot can be improved by highlighting these feature interactions.</p>
</div>
<div id="shap-interaction-values" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.8</span> SHAP Interaction Values<a href="shap.html#shap-interaction-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The interaction effect is the additional combined feature effect after accounting for the individual feature effects.
The Shapley interaction index from game theory is defined as:</p>
<p><span class="math display">\[\phi_{i,j}=\sum_{S\subseteq\setminus\{i,j\}}\frac{|S|!(M-|S|-2)!}{2(M-1)!}\delta_{ij}(S)\]</span></p>
<p>when <span class="math inline">\(i\neq{}j\)</span> and:</p>
<p><span class="math display">\[\delta_{ij}(S)=\hat{f}_x(S\cup\{i,j\})-\hat{f}_x(S\cup\{i\})-\hat{f}_x(S\cup\{j\})+\hat{f}_x(S)\]</span></p>
<p>This formula subtracts the main effect of the features so that we get the pure interaction effect after accounting for the individual effects.
We average the values over all possible feature coalitions S, as in the Shapley value computation.
When we compute SHAP interaction values for all features, we get one matrix per instance with dimensions M x M, where M is the number of features.</p>
<p>How can we use the interaction index?
For example, to automatically color the SHAP feature dependence plot with the strongest interaction:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-48"></span>
<img src="images/shap-dependence-interaction.png" alt="SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of a STD increases the predicted cancer risk. For more years on contraceptives, the occurence of a STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits)." width="\textwidth" />
<p class="caption">
FIGURE 9.28: SHAP feature dependence plot with interaction visualization. Years on hormonal contraceptives interacts with STDs. In cases close to 0 years, the occurence of a STD increases the predicted cancer risk. For more years on contraceptives, the occurence of a STD reduces the predicted risk. Again, this is not a causal model. Effects might be due to confounding (e.g. STDs and lower cancer risk could be correlated with more doctor visits).
</p>
</div>
</div>
<div id="clustering-shapley-values" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.9</span> Clustering Shapley Values<a href="shap.html#clustering-shapley-values" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can cluster your data with the help of Shapley values.
The goal of clustering is to find groups of similar instances.
Normally, clustering is based on features.
Features are often on different scales.
For example, height might be measured in meters, color intensity from 0 to 100 and some sensor output between -1 and 1.
The difficulty is to compute distances between instances with such different, non-comparable features.</p>
<p>SHAP clustering works by clustering the Shapley values of each instance.
This means that you cluster instances by explanation similarity.
All SHAP values have the same unit – the unit of the prediction space.
You can use any clustering method.
The following example uses hierarchical agglomerative clustering to order the instances.</p>
<p>The plot consists of many force plots, each of which explains the prediction of an instance.
We rotate the force plots vertically and place them side by side according to their clustering similarity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-49"></span>
<img src="images/shap-clustering.png" alt="Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. One cluster stands out: On the right is a group with a high predicted cancer risk." width="\textwidth" />
<p class="caption">
FIGURE 9.29: Stacked SHAP explanations clustered by explanation similarity. Each position on the x-axis is an instance of the data. Red SHAP values increase the prediction, blue values decrease it. One cluster stands out: On the right is a group with a high predicted cancer risk.
</p>
</div>
</div>
<div id="advantages-17" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.10</span> Advantages<a href="shap.html#advantages-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since SHAP computes Shapley values, all the advantages of Shapley values apply:
SHAP has a <strong>solid theoretical foundation</strong> in game theory.
The prediction is <strong>fairly distributed</strong> among the feature values.
We get <strong>contrastive explanations</strong> that compare the prediction with the average prediction.</p>
<p>SHAP <strong>connects LIME and Shapley values</strong>.
This is very useful to better understand both methods.
It also helps to unify the field of interpretable machine learning.</p>
<p>SHAP has a <strong>fast implementation for tree-based models</strong>.
I believe this was key to the popularity of SHAP, because the biggest barrier for adoption of Shapley values is the slow computation.</p>
<p>The fast computation makes it possible to compute the many Shapley values needed for the <strong>global model interpretations</strong>.
The global interpretation methods include feature importance, feature dependence, interactions, clustering and summary plots.
With SHAP, global interpretations are consistent with the local explanations, since the Shapley values are the “atomic unit” of the global interpretations.
If you use LIME for local explanations and partial dependence plots plus permutation feature importance for global explanations, you lack a common foundation.</p>
</div>
<div id="disadvantages-17" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.11</span> Disadvantages<a href="shap.html#disadvantages-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>KernelSHAP is slow</strong>.
This makes KernelSHAP impractical to use when you want to compute Shapley values for many instances.
Also all global SHAP methods such as SHAP feature importance require computing Shapley values for a lot of instances.</p>
<p><strong>KernelSHAP ignores feature dependence</strong>.
Most other permutation based interpretation methods have this problem.
By replacing feature values with values from random instances, it is usually easier to randomly sample from the marginal distribution.
However, if features are dependent, e.g. correlated, this leads to putting too much weight on unlikely data points.
TreeSHAP solves this problem by explicitly modeling the conditional expected prediction.</p>
<p><strong>TreeSHAP can produce unintuitive feature attributions</strong>.
While TreeSHAP solves the problem of extrapolating to unlikely data points, it does so by changing the value function and therefore slightly changes the game.
TreeSHAP changes the value function by relying on the conditional expected prediction.
With the change in the value function, features that have no influence on the prediction can get a TreeSHAP value different from zero.</p>
<p>The disadvantages of Shapley values also apply to SHAP:
Shapley values <strong>can be misinterpreted</strong> and access to data is needed to compute them for new data (except for TreeSHAP).</p>
<p>It is <strong>possible to create intentionally misleading interpretations</strong> with SHAP, which can hide biases <a href="#fn72" class="footnote-ref" id="fnref72"><sup>72</sup></a>.
If you are the data scientist creating the explanations, this is not an actual problem (it would even be an advantage if you are the evil data scientist who wants to create misleading explanations).
For the receivers of a SHAP explanation, it is a disadvantage: they cannot be sure about the truthfulness of the explanation.</p>
</div>
<div id="software-5" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.12</span> Software<a href="shap.html#software-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The authors implemented SHAP in the <a href="https://github.com/slundberg/shap">shap</a> Python package.
This implementation works for tree-based models in the <a href="https://scikit-learn.org/stable/">scikit-learn</a> machine learning library for Python.
The shap package was also used for the examples in this chapter.
SHAP is integrated into the tree boosting frameworks <a href="https://github.com/dmlc/xgboost/tree/master/python-package">xgboost</a> and <a href="https://github.com/microsoft/LightGBM">LightGBM</a>.
In R, there are the <a href="https://modeloriented.github.io/shapper/">shapper</a> and <a href="https://github.com/bgreenwell/fastshap">fastshap</a> packages.
SHAP is also included in the R <a href="https://rdrr.io/cran/xgboost/man/xgb.plot.shap.html">xgboost</a> package.</p>

</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="69">
<li id="fn69"><p>Lundberg, Scott M., and Su-In Lee. “A unified approach to interpreting model predictions.” Advances in Neural Information Processing Systems (2017).<a href="shap.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn70"><p>Sundararajan, Mukund, and Amir Najmi. “The many Shapley values for model explanation.” arXiv preprint arXiv:1908.08474 (2019).<a href="shap.html#fnref70" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>Janzing, Dominik, Lenon Minorics, and Patrick Blöbaum. “Feature relevance quantification in explainable AI: A causal problem.” International Conference on Artificial Intelligence and Statistics. PMLR (2020).<a href="shap.html#fnref71" class="footnote-back">↩︎</a></p></li>
<li id="fn72"><p>Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling lime and shap: Adversarial attacks on post hoc explanation methods.” In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 180-186 (2020).<a href="shap.html#fnref72" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="shapley.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="neural-networks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/05.9b-agnostic-shap.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["interpretable-ml.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
